#!/usr/bin/env python3
"""
Script para descargar modelo una vez y guardarlo localmente
Ejecutar CON INTERNET antes de usar offline
"""

import sys
import os
import json
import time
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def download_model(model_name="deepseek-ai/deepseek-coder-1.3b-instruct", 
                   cache_dir="./models"):
    """
    Descarga el modelo y lo guarda localmente
    
    Args:
        model_name: Nombre del modelo en HuggingFace
        cache_dir: Directorio donde guardar
    """
    
    print("=" * 60)
    print("üì• DESCARGADOR DE MODELO DeepSeek")
    print("=" * 60)
    print(f"Modelo: {model_name}")
    print(f"Guardar en: {cache_dir}")
    print("=" * 60)
    
    # Crear directorio
    cache_path = Path(cache_dir)
    cache_path.mkdir(parents=True, exist_ok=True)
    
    # Configurar para descarga completa
    os.environ['TRANSFORMERS_OFFLINE'] = '0'
    os.environ['HF_HUB_OFFLINE'] = '0'
    
    try:
        # Paso 1: Descargar tokenizador
        print("\n1Ô∏è‚É£ Descargando tokenizador...")
        start_time = time.time()
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=str(cache_path),
            local_files_only=False,
            trust_remote_code=True
        )
        
        # Forzar guardado completo
        tokenizer.save_pretrained(cache_path)
        
        tokenizer_time = time.time() - start_time
        print(f"   ‚úÖ Tokenizador descargado ({tokenizer_time:.1f}s)")
        
        # Paso 2: Descargar modelo
        print("\n2Ô∏è‚É£ Descargando modelo (esto puede tomar varios minutos)...")
        start_time = time.time()
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=str(cache_path),
            local_files_only=False,
            trust_remote_code=True,
            torch_dtype=torch.float32
        )
        
        # Forzar guardado completo
        model.save_pretrained(cache_path, safe_serialization=True)
        
        model_time = time.time() - start_time
        print(f"   ‚úÖ Modelo descargado ({model_time:.1f}s)")
        
        # Paso 3: Verificar archivos descargados
        print("\n3Ô∏è‚É£ Verificando archivos descargados...")
        files = list(cache_path.glob("**/*"))
        print(f"   üìÅ {len(files)} archivos en total")
        
        # Mostrar archivos principales
        essential_files = ["config.json", "tokenizer_config.json", 
                          "model.safetensors", "pytorch_model.bin"]
        
        for file in essential_files:
            if list(cache_path.glob(f"**/{file}")):
                print(f"   ‚úÖ {file}")
            else:
                print(f"   ‚ö†Ô∏è  {file} (no encontrado)")
        
        # Crear archivo de verificaci√≥n
        verification = {
            "model": model_name,
            "downloaded_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "cache_path": str(cache_path.absolute()),
            "tokenizer_time": tokenizer_time,
            "model_time": model_time,
            "total_time": tokenizer_time + model_time,
            "status": "complete"
        }
        
        with open(cache_path / "download_info.json", 'w', encoding='utf-8') as f:
            json.dump(verification, f, indent=2, ensure_ascii=False)
        
        print("\n" + "=" * 60)
        print("üéâ ¬°DESCARGA COMPLETADA!")
        print("=" * 60)
        print(f"Total tiempo: {verification['total_time']:.1f} segundos")
        print(f"Modelo guardado en: {cache_path.absolute()}")
        print("\nAhora puedes usar el programa SIN INTERNET.")
        print("Ejecuta: python main.py")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå ERROR en la descarga: {str(e)}")
        print("\nPosibles soluciones:")
        print("1. Verifica tu conexi√≥n a internet")
        print("2. Aseg√∫rate de tener suficiente espacio en disco (al menos 5GB)")
        print("3. Intenta con un modelo m√°s peque√±o")
        print("4. Revisa que Python y pip est√©n actualizados")
        
        # Guardar error
        error_info = {
            "model": model_name,
            "error": str(e),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(cache_path / "download_error.json", 'w', encoding='utf-8') as f:
            json.dump(error_info, f, indent=2, ensure_ascii=False)
        
        return False

def show_menu():
    """Muestra men√∫ de modelos disponibles"""
    print("\nüìã MODELOS DISPONIBLES:")
    print("1. deepseek-ai/deepseek-coder-1.3b-instruct (Recomendado - 2.7GB)")
    print("2. microsoft/phi-2 (Alternativa peque√±a - 2.7GB)")
    print("3. google/gemma-2b (Alternativa buena - 2.5GB)")
    print("4. Otro (ingresa nombre completo)")
    print("0. Salir")
    
    choice = input("\nSelecciona opci√≥n (1-4): ").strip()
    
    models = {
        "1": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "2": "microsoft/phi-2",
        "3": "google/gemma-2b"
    }
    
    if choice == "0":
        print("Saliendo...")
        sys.exit(0)
    elif choice in models:
        return models[choice]
    elif choice == "4":
        custom_model = input("Ingresa el nombre del modelo en HuggingFace: ").strip()
        if custom_model:
            return custom_model
        else:
            print("Nombre inv√°lido, usando opci√≥n por defecto")
            return "deepseek-ai/deepseek-coder-1.3b-instruct"
    else:
        print("Opci√≥n inv√°lida, usando modelo por defecto")
        return "deepseek-ai/deepseek-coder-1.3b-instruct"

if __name__ == "__main__":
    print("ü§ñ DESCARGA INICIAL DE MODELO")
    print("Este script descarga el modelo UNA VEZ con internet")
    print("Luego podr√°s usar el programa sin conexi√≥n")
    print("\n" + "=" * 60)
    
    # Mostrar men√∫
    model_name = show_menu()
    
    # Confirmar
    print(f"\n¬øDescargar {model_name}?")
    print("Necesitas aproximadamente 5GB de espacio libre.")
    confirm = input("Continuar? (s/n): ").strip().lower()
    
    if confirm != 's':
        print("Descarga cancelada")
        sys.exit(0)
    
    # Descargar
    success = download_model(model_name)
    
    if success:
        print("\nüéØ Instrucciones para usar sin internet:")
        print("1. Edita config.json y aseg√∫rate que tenga:")
        print('   "model": "{}"'.format(model_name))
        print('   "offline_mode": true')
        print("2. Ejecuta: python main.py")
        print("3. ¬°Disfruta de tu IA local!")
    else:
        print("\n‚ùå La descarga fall√≥. Revisa los mensajes de error.")
    
    input("\nPresiona Enter para salir...")